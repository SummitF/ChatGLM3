{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {},
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {},
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {},
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6278d26-0623-467d-b167-a969d4c11ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 19:09:04,749 - modelscope - INFO - PyTorch version 2.1.2 Found.\n",
      "2024-03-31 19:09:04,753 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-03-31 19:09:04,790 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 89950594ec4c84f3a9a308ac70d556a6 and a total number of 972 components indexed\n",
      "2024-03-31 19:09:05,871 - modelscope - INFO - Use user-specified model revision: v1.0.2\n",
      "Downloading: 100%|██████████| 37.0/37.0 [00:00<00:00, 22.4kB/s]\n",
      "Downloading: 100%|██████████| 4.37k/4.37k [00:00<00:00, 426kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"ZhipuAI/chatglm3-6b\", revision = \"v1.0.2\", cache_dir=\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:08<00:00,  1.19s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 420166.61 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 85861.70 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 83807.75 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:02<00:00, 57180.61 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1232.98 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1227.11 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.7859, 'learning_rate': 4.99e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.6332, 'learning_rate': 4.9800000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3904, 'learning_rate': 4.97e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.1148, 'learning_rate': 4.96e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.9002, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9221, 'learning_rate': 4.94e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.8031, 'learning_rate': 4.93e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.8203, 'learning_rate': 4.92e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6959, 'learning_rate': 4.91e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6988, 'learning_rate': 4.9e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.7334, 'learning_rate': 4.89e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6982, 'learning_rate': 4.88e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6738, 'learning_rate': 4.87e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.7137, 'learning_rate': 4.86e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.7059, 'learning_rate': 4.85e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6738, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5865, 'learning_rate': 4.83e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6717, 'learning_rate': 4.82e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5729, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.627, 'learning_rate': 4.8e-05, 'epoch': 0.01}                        \n",
      "{'loss': 3.7205, 'learning_rate': 4.79e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.665, 'learning_rate': 4.78e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.5424, 'learning_rate': 4.77e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.6055, 'learning_rate': 4.76e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.6451, 'learning_rate': 4.75e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5486, 'learning_rate': 4.74e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.6527, 'learning_rate': 4.73e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.7102, 'learning_rate': 4.72e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5549, 'learning_rate': 4.71e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5867, 'learning_rate': 4.7e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.5754, 'learning_rate': 4.69e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5654, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.642, 'learning_rate': 4.6700000000000003e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5514, 'learning_rate': 4.660000000000001e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.4797, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.6381, 'learning_rate': 4.64e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5314, 'learning_rate': 4.630000000000001e-05, 'epoch': 0.03}         \n",
      "{'loss': 3.5607, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.6754, 'learning_rate': 4.61e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5559, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.03}         \n",
      "{'loss': 3.4416, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.5529, 'learning_rate': 4.58e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.6316, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.507, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.03}         \n",
      "{'loss': 3.4748, 'learning_rate': 4.55e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5693, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.643, 'learning_rate': 4.53e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.5104, 'learning_rate': 4.52e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5488, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.543, 'learning_rate': 4.5e-05, 'epoch': 0.03}                        \n",
      " 10%|███▊                                  | 500/5000 [09:08<1:24:44,  1.13s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.02s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.85s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:26<00:00,  8.20s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.627 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.558032000000004, 'eval_rouge-2': 6.944923999999999, 'eval_rouge-l': 24.143843999999994, 'eval_bleu-4': 0.03339720765083169, 'eval_runtime': 40.9794, 'eval_samples_per_second': 1.22, 'eval_steps_per_second': 0.098, 'epoch': 0.03}\n",
      " 10%|███▊                                  | 500/5000 [09:49<1:24:44,  1.13s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  8.20s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-500\n",
      "tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.5754, 'learning_rate': 4.49e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.6393, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.04}        \n",
      "{'loss': 3.4811, 'learning_rate': 4.47e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.5529, 'learning_rate': 4.46e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.6182, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.04}        \n",
      "{'loss': 3.5627, 'learning_rate': 4.44e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.5234, 'learning_rate': 4.43e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.4564, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.04}        \n",
      "{'loss': 3.5701, 'learning_rate': 4.41e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.5023, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.04}        \n",
      "{'loss': 3.4092, 'learning_rate': 4.39e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.4955, 'learning_rate': 4.38e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.4762, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.04}        \n",
      "{'loss': 3.5127, 'learning_rate': 4.36e-05, 'epoch': 0.04}                      \n",
      "{'loss': 3.5584, 'learning_rate': 4.35e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.5863, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.4654, 'learning_rate': 4.33e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.448, 'learning_rate': 4.32e-05, 'epoch': 0.05}                       \n",
      "{'loss': 3.5754, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.4953, 'learning_rate': 4.3e-05, 'epoch': 0.05}                       \n",
      "{'loss': 3.4266, 'learning_rate': 4.29e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.4881, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.4385, 'learning_rate': 4.27e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.4846, 'learning_rate': 4.26e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.5678, 'learning_rate': 4.25e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.4756, 'learning_rate': 4.24e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.5531, 'learning_rate': 4.23e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.5557, 'learning_rate': 4.22e-05, 'epoch': 0.05}                      \n",
      "{'loss': 3.5805, 'learning_rate': 4.21e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5709, 'learning_rate': 4.2e-05, 'epoch': 0.06}                       \n",
      "{'loss': 3.4814, 'learning_rate': 4.19e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5162, 'learning_rate': 4.18e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.4527, 'learning_rate': 4.17e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5398, 'learning_rate': 4.16e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5594, 'learning_rate': 4.15e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.6055, 'learning_rate': 4.14e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5205, 'learning_rate': 4.13e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5654, 'learning_rate': 4.12e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.4787, 'learning_rate': 4.11e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.4896, 'learning_rate': 4.1e-05, 'epoch': 0.06}                       \n",
      "{'loss': 3.4715, 'learning_rate': 4.09e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5816, 'learning_rate': 4.08e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.5568, 'learning_rate': 4.07e-05, 'epoch': 0.06}                      \n",
      "{'loss': 3.4318, 'learning_rate': 4.0600000000000004e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.4998, 'learning_rate': 4.05e-05, 'epoch': 0.07}                      \n",
      "{'loss': 3.5514, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.5111, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.434, 'learning_rate': 4.02e-05, 'epoch': 0.07}                       \n",
      "{'loss': 3.4301, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.5648, 'learning_rate': 4e-05, 'epoch': 0.07}                         \n",
      " 20%|███████▍                             | 1000/5000 [18:49<1:11:42,  1.08s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.30s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:06,  6.08s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.365778, 'eval_rouge-2': 6.1511439999999995, 'eval_rouge-l': 23.406216, 'eval_bleu-4': 0.030123143709252155, 'eval_runtime': 30.8647, 'eval_samples_per_second': 1.62, 'eval_steps_per_second': 0.13, 'epoch': 0.07}\n",
      " 20%|███████▍                             | 1000/5000 [19:19<1:11:42,  1.08s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  4.49s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-1000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-1000/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.5621, 'learning_rate': 3.99e-05, 'epoch': 0.07}                      \n",
      "{'loss': 3.6314, 'learning_rate': 3.9800000000000005e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.4535, 'learning_rate': 3.97e-05, 'epoch': 0.07}                      \n",
      "{'loss': 3.5529, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.07}         \n",
      "{'loss': 3.5043, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.5543, 'learning_rate': 3.94e-05, 'epoch': 0.07}                      \n",
      "{'loss': 3.4248, 'learning_rate': 3.9300000000000007e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.5773, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.08}        \n",
      "{'loss': 3.5182, 'learning_rate': 3.91e-05, 'epoch': 0.08}                      \n",
      "{'loss': 3.5066, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.08}        \n",
      "{'loss': 3.552, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.08}         \n",
      "{'loss': 3.5113, 'learning_rate': 3.88e-05, 'epoch': 0.08}                      \n",
      "{'loss': 3.4148, 'learning_rate': 3.8700000000000006e-05, 'epoch': 0.08}        \n",
      "{'loss': 3.5693, 'learning_rate': 3.86e-05, 'epoch': 0.08}                      \n",
      "{'loss': 3.4438, 'learning_rate': 3.85e-05, 'epoch': 0.08}                      \n",
      "{'loss': 3.4311, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.08}        \n",
      "{'loss': 3.477, 'learning_rate': 3.83e-05, 'epoch': 0.08}                       \n",
      "{'loss': 3.4293, 'learning_rate': 3.82e-05, 'epoch': 0.08}                      \n",
      "{'loss': 3.5416, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.08}        \n",
      "{'loss': 3.4576, 'learning_rate': 3.8e-05, 'epoch': 0.08}                       \n",
      "{'loss': 3.4783, 'learning_rate': 3.79e-05, 'epoch': 0.08}                      \n",
      "{'loss': 3.4824, 'learning_rate': 3.7800000000000004e-05, 'epoch': 0.09}        \n",
      "{'loss': 3.4059, 'learning_rate': 3.77e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.4758, 'learning_rate': 3.76e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.5498, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.09}        \n",
      "{'loss': 3.4266, 'learning_rate': 3.74e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.4174, 'learning_rate': 3.73e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.5, 'learning_rate': 3.72e-05, 'epoch': 0.09}                         \n",
      "{'loss': 3.5922, 'learning_rate': 3.71e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.5279, 'learning_rate': 3.7e-05, 'epoch': 0.09}                       \n",
      "{'loss': 3.4951, 'learning_rate': 3.69e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.5854, 'learning_rate': 3.68e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.4914, 'learning_rate': 3.6700000000000004e-05, 'epoch': 0.09}        \n",
      "{'loss': 3.4408, 'learning_rate': 3.66e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.5742, 'learning_rate': 3.65e-05, 'epoch': 0.09}                      \n",
      "{'loss': 3.4521, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.09}        \n",
      "{'loss': 3.4986, 'learning_rate': 3.63e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.5449, 'learning_rate': 3.62e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.4141, 'learning_rate': 3.61e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.3766, 'learning_rate': 3.6e-05, 'epoch': 0.1}                        \n",
      "{'loss': 3.5063, 'learning_rate': 3.59e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.527, 'learning_rate': 3.58e-05, 'epoch': 0.1}                        \n",
      "{'loss': 3.4861, 'learning_rate': 3.57e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.5492, 'learning_rate': 3.56e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.4283, 'learning_rate': 3.55e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.3803, 'learning_rate': 3.54e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.4162, 'learning_rate': 3.53e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.392, 'learning_rate': 3.52e-05, 'epoch': 0.1}                        \n",
      "{'loss': 3.5607, 'learning_rate': 3.51e-05, 'epoch': 0.1}                       \n",
      "{'loss': 3.5191, 'learning_rate': 3.5e-05, 'epoch': 0.1}                        \n",
      " 30%|███████████▋                           | 1500/5000 [28:19<58:49,  1.01s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.23s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:06,  6.01s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.857126000000004, 'eval_rouge-2': 6.947582000000001, 'eval_rouge-l': 23.16058, 'eval_bleu-4': 0.03350708617528114, 'eval_runtime': 41.0739, 'eval_samples_per_second': 1.217, 'eval_steps_per_second': 0.097, 'epoch': 0.1}\n",
      " 30%|███████████▋                           | 1500/5000 [29:00<58:49,  1.01s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  8.32s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-1500\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-1500/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.5406, 'learning_rate': 3.49e-05, 'epoch': 0.11}                      \n",
      "{'loss': 3.4117, 'learning_rate': 3.48e-05, 'epoch': 0.11}                      \n",
      "{'loss': 3.5809, 'learning_rate': 3.4699999999999996e-05, 'epoch': 0.11}        \n",
      "{'loss': 3.4256, 'learning_rate': 3.46e-05, 'epoch': 0.11}                      \n",
      "{'loss': 3.4125, 'learning_rate': 3.45e-05, 'epoch': 0.11}                      \n",
      "{'loss': 3.4541, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.11}        \n",
      "{'loss': 3.4498, 'learning_rate': 3.430000000000001e-05, 'epoch': 0.11}         \n",
      "{'loss': 3.5727, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.11}        \n",
      "{'loss': 3.465, 'learning_rate': 3.41e-05, 'epoch': 0.11}                       \n",
      "{'loss': 3.5055, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.11}        \n",
      "{'loss': 3.4979, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.11}        \n",
      "{'loss': 3.458, 'learning_rate': 3.38e-05, 'epoch': 0.11}                       \n",
      "{'loss': 3.508, 'learning_rate': 3.3700000000000006e-05, 'epoch': 0.11}         \n",
      "{'loss': 3.4707, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.11}        \n",
      "{'loss': 3.4736, 'learning_rate': 3.35e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.4117, 'learning_rate': 3.3400000000000005e-05, 'epoch': 0.12}        \n",
      "{'loss': 3.4953, 'learning_rate': 3.33e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.4783, 'learning_rate': 3.32e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.4701, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.12}        \n",
      "{'loss': 3.434, 'learning_rate': 3.3e-05, 'epoch': 0.12}                        \n",
      "{'loss': 3.3686, 'learning_rate': 3.29e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.4375, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.12}        \n",
      "{'loss': 3.4537, 'learning_rate': 3.27e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.3799, 'learning_rate': 3.26e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.5549, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.12}        \n",
      "{'loss': 3.4693, 'learning_rate': 3.24e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.4182, 'learning_rate': 3.2300000000000006e-05, 'epoch': 0.12}        \n",
      "{'loss': 3.4137, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.12}        \n",
      "{'loss': 3.4932, 'learning_rate': 3.21e-05, 'epoch': 0.12}                      \n",
      "{'loss': 3.3283, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.13}        \n",
      "{'loss': 3.5326, 'learning_rate': 3.19e-05, 'epoch': 0.13}                      \n",
      "{'loss': 3.4062, 'learning_rate': 3.18e-05, 'epoch': 0.13}                      \n",
      "{'loss': 3.3721, 'learning_rate': 3.1700000000000005e-05, 'epoch': 0.13}        \n",
      "{'loss': 3.4645, 'learning_rate': 3.16e-05, 'epoch': 0.13}                      \n",
      "{'loss': 3.509, 'learning_rate': 3.15e-05, 'epoch': 0.13}                       \n",
      "{'loss': 3.4867, 'learning_rate': 3.1400000000000004e-05, 'epoch': 0.13}        \n",
      "{'loss': 3.407, 'learning_rate': 3.13e-05, 'epoch': 0.13}                       \n",
      "{'loss': 3.3258, 'learning_rate': 3.12e-05, 'epoch': 0.13}                      \n",
      "{'loss': 3.3727, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.13}        \n",
      "{'loss': 3.4902, 'learning_rate': 3.1e-05, 'epoch': 0.13}                       \n",
      "{'loss': 3.3898, 'learning_rate': 3.09e-05, 'epoch': 0.13}                      \n",
      "{'loss': 3.4709, 'learning_rate': 3.08e-05, 'epoch': 0.13}                      \n",
      "{'loss': 3.4232, 'learning_rate': 3.07e-05, 'epoch': 0.13}                      \n",
      "{'loss': 3.5936, 'learning_rate': 3.06e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.5447, 'learning_rate': 3.05e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.4162, 'learning_rate': 3.04e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.5674, 'learning_rate': 3.03e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.4426, 'learning_rate': 3.02e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.5303, 'learning_rate': 3.01e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.534, 'learning_rate': 3e-05, 'epoch': 0.14}                          \n",
      " 40%|███████████████▌                       | 2000/5000 [37:59<52:24,  1.05s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.37s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:25<00:08,  8.98s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.047974, 'eval_rouge-2': 7.3372920000000015, 'eval_rouge-l': 24.771374, 'eval_bleu-4': 0.033610918866698106, 'eval_runtime': 40.7175, 'eval_samples_per_second': 1.228, 'eval_steps_per_second': 0.098, 'epoch': 0.14}\n",
      " 40%|███████████████▌                       | 2000/5000 [38:40<52:24,  1.05s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  6.31s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-2000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-2000/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4172, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.14}        \n",
      "{'loss': 3.5195, 'learning_rate': 2.98e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.4076, 'learning_rate': 2.97e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.4141, 'learning_rate': 2.96e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.5039, 'learning_rate': 2.95e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.4572, 'learning_rate': 2.94e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.4697, 'learning_rate': 2.93e-05, 'epoch': 0.14}                      \n",
      "{'loss': 3.4576, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.4105, 'learning_rate': 2.91e-05, 'epoch': 0.15}                      \n",
      "{'loss': 3.5488, 'learning_rate': 2.9e-05, 'epoch': 0.15}                       \n",
      "{'loss': 3.4561, 'learning_rate': 2.8899999999999998e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.509, 'learning_rate': 2.88e-05, 'epoch': 0.15}                       \n",
      "{'loss': 3.4936, 'learning_rate': 2.87e-05, 'epoch': 0.15}                      \n",
      "{'loss': 3.3164, 'learning_rate': 2.86e-05, 'epoch': 0.15}                      \n",
      "{'loss': 3.4582, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.4039, 'learning_rate': 2.84e-05, 'epoch': 0.15}                      \n",
      "{'loss': 3.4096, 'learning_rate': 2.83e-05, 'epoch': 0.15}                      \n",
      "{'loss': 3.4008, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.4842, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.3666, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.4301, 'learning_rate': 2.7900000000000004e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.3943, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.4176, 'learning_rate': 2.7700000000000002e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.5189, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.4699, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.4381, 'learning_rate': 2.7400000000000002e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.4775, 'learning_rate': 2.7300000000000003e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.3764, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.4596, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.4316, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.434, 'learning_rate': 2.6900000000000003e-05, 'epoch': 0.16}         \n",
      "{'loss': 3.4668, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.4744, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.5924, 'learning_rate': 2.6600000000000003e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.5357, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.4988, 'learning_rate': 2.64e-05, 'epoch': 0.16}                      \n",
      "{'loss': 3.4674, 'learning_rate': 2.6300000000000002e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.4895, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.4465, 'learning_rate': 2.61e-05, 'epoch': 0.17}                      \n",
      "{'loss': 3.5199, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.5342, 'learning_rate': 2.5900000000000003e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.332, 'learning_rate': 2.58e-05, 'epoch': 0.17}                       \n",
      "{'loss': 3.3844, 'learning_rate': 2.57e-05, 'epoch': 0.17}                      \n",
      "{'loss': 3.4436, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.4748, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.4412, 'learning_rate': 2.54e-05, 'epoch': 0.17}                      \n",
      "{'loss': 3.3691, 'learning_rate': 2.5300000000000002e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.4799, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.17}        \n",
      "{'loss': 3.4279, 'learning_rate': 2.51e-05, 'epoch': 0.17}                      \n",
      "{'loss': 3.3139, 'learning_rate': 2.5e-05, 'epoch': 0.17}                       \n",
      " 50%|███████████████████▌                   | 2500/5000 [47:40<43:17,  1.04s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.40s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:25<00:09,  9.01s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.196812, 'eval_rouge-2': 6.583404000000001, 'eval_rouge-l': 22.922178, 'eval_bleu-4': 0.030910135907127344, 'eval_runtime': 51.157, 'eval_samples_per_second': 0.977, 'eval_steps_per_second': 0.078, 'epoch': 0.17}\n",
      " 50%|███████████████████▌                   | 2500/5000 [48:31<43:17,  1.04s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:37<00:00, 10.18s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-2500\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-2500/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4822, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.18}        \n",
      "{'loss': 3.4062, 'learning_rate': 2.48e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.474, 'learning_rate': 2.47e-05, 'epoch': 0.18}                       \n",
      "{'loss': 3.4986, 'learning_rate': 2.46e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.4857, 'learning_rate': 2.45e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.4465, 'learning_rate': 2.44e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.3551, 'learning_rate': 2.43e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.4195, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.18}        \n",
      "{'loss': 3.4225, 'learning_rate': 2.41e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.4924, 'learning_rate': 2.4e-05, 'epoch': 0.18}                       \n",
      "{'loss': 3.492, 'learning_rate': 2.39e-05, 'epoch': 0.18}                       \n",
      "{'loss': 3.5195, 'learning_rate': 2.38e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.3551, 'learning_rate': 2.37e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.2506, 'learning_rate': 2.36e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.3395, 'learning_rate': 2.35e-05, 'epoch': 0.18}                      \n",
      "{'loss': 3.409, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.19}         \n",
      "{'loss': 3.3889, 'learning_rate': 2.3300000000000004e-05, 'epoch': 0.19}        \n",
      "{'loss': 3.3662, 'learning_rate': 2.32e-05, 'epoch': 0.19}                      \n",
      "{'loss': 3.4418, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.19}        \n",
      "{'loss': 3.3607, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.19}        \n",
      "{'loss': 3.492, 'learning_rate': 2.29e-05, 'epoch': 0.19}                       \n",
      "{'loss': 3.3977, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.19}        \n",
      "{'loss': 3.5039, 'learning_rate': 2.2700000000000003e-05, 'epoch': 0.19}        \n",
      "{'loss': 3.3518, 'learning_rate': 2.26e-05, 'epoch': 0.19}                      \n",
      "{'loss': 3.415, 'learning_rate': 2.25e-05, 'epoch': 0.19}                       \n",
      "{'loss': 3.4764, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.19}        \n",
      "{'loss': 3.5363, 'learning_rate': 2.23e-05, 'epoch': 0.19}                      \n",
      "{'loss': 3.3857, 'learning_rate': 2.22e-05, 'epoch': 0.19}                      \n",
      "{'loss': 3.3934, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.19}        \n",
      "{'loss': 3.434, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.2}          \n",
      "{'loss': 3.4275, 'learning_rate': 2.19e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.4488, 'learning_rate': 2.18e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.4992, 'learning_rate': 2.1700000000000002e-05, 'epoch': 0.2}         \n",
      "{'loss': 3.4721, 'learning_rate': 2.16e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.4371, 'learning_rate': 2.15e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.4807, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.2}         \n",
      "{'loss': 3.491, 'learning_rate': 2.13e-05, 'epoch': 0.2}                        \n",
      "{'loss': 3.5076, 'learning_rate': 2.12e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.4115, 'learning_rate': 2.11e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.5156, 'learning_rate': 2.1e-05, 'epoch': 0.2}                        \n",
      "{'loss': 3.3645, 'learning_rate': 2.09e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.3687, 'learning_rate': 2.08e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.3611, 'learning_rate': 2.07e-05, 'epoch': 0.2}                       \n",
      "{'loss': 3.3666, 'learning_rate': 2.06e-05, 'epoch': 0.21}                      \n",
      "{'loss': 3.3391, 'learning_rate': 2.05e-05, 'epoch': 0.21}                      \n",
      "{'loss': 3.5906, 'learning_rate': 2.04e-05, 'epoch': 0.21}                      \n",
      "{'loss': 3.4838, 'learning_rate': 2.0300000000000002e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.5332, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.4703, 'learning_rate': 2.01e-05, 'epoch': 0.21}                      \n",
      "{'loss': 3.5, 'learning_rate': 2e-05, 'epoch': 0.21}                            \n",
      " 60%|███████████████████████▍               | 3000/5000 [57:36<38:22,  1.15s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.41s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:25<00:09,  9.02s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.597644, 'eval_rouge-2': 7.136498, 'eval_rouge-l': 23.686422000000004, 'eval_bleu-4': 0.031670016616263494, 'eval_runtime': 40.6026, 'eval_samples_per_second': 1.231, 'eval_steps_per_second': 0.099, 'epoch': 0.21}\n",
      " 60%|███████████████████████▍               | 3000/5000 [58:17<38:22,  1.15s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  6.25s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-3000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-3000/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4051, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.3043, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.4447, 'learning_rate': 1.97e-05, 'epoch': 0.21}                      \n",
      "{'loss': 3.4707, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.4672, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.3965, 'learning_rate': 1.94e-05, 'epoch': 0.21}                      \n",
      "{'loss': 3.3846, 'learning_rate': 1.93e-05, 'epoch': 0.21}                      \n",
      "{'loss': 3.5086, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.22}        \n",
      "{'loss': 3.4582, 'learning_rate': 1.91e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.3672, 'learning_rate': 1.9e-05, 'epoch': 0.22}                       \n",
      "{'loss': 3.4064, 'learning_rate': 1.8900000000000002e-05, 'epoch': 0.22}        \n",
      "{'loss': 3.4727, 'learning_rate': 1.88e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.4619, 'learning_rate': 1.87e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.3811, 'learning_rate': 1.86e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.4338, 'learning_rate': 1.85e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.5139, 'learning_rate': 1.84e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.4766, 'learning_rate': 1.83e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.5273, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.22}        \n",
      "{'loss': 3.5365, 'learning_rate': 1.81e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.4582, 'learning_rate': 1.8e-05, 'epoch': 0.22}                       \n",
      "{'loss': 3.4463, 'learning_rate': 1.79e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.4996, 'learning_rate': 1.78e-05, 'epoch': 0.22}                      \n",
      "{'loss': 3.4062, 'learning_rate': 1.77e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.5354, 'learning_rate': 1.76e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.4453, 'learning_rate': 1.75e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.4307, 'learning_rate': 1.74e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.3904, 'learning_rate': 1.73e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.3994, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.23}        \n",
      "{'loss': 3.4902, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.23}        \n",
      "{'loss': 3.5264, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.23}        \n",
      "{'loss': 3.4062, 'learning_rate': 1.69e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.3939, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.23}        \n",
      "{'loss': 3.3564, 'learning_rate': 1.6700000000000003e-05, 'epoch': 0.23}        \n",
      "{'loss': 3.4029, 'learning_rate': 1.66e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.4238, 'learning_rate': 1.65e-05, 'epoch': 0.23}                      \n",
      "{'loss': 3.5096, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.23}        \n",
      "{'loss': 3.4252, 'learning_rate': 1.63e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.4381, 'learning_rate': 1.62e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.4373, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.24}        \n",
      "{'loss': 3.3674, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.24}        \n",
      "{'loss': 3.4912, 'learning_rate': 1.59e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.4357, 'learning_rate': 1.58e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.5453, 'learning_rate': 1.5700000000000002e-05, 'epoch': 0.24}        \n",
      "{'loss': 3.4139, 'learning_rate': 1.56e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.3291, 'learning_rate': 1.55e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.4268, 'learning_rate': 1.54e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.4311, 'learning_rate': 1.53e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.3893, 'learning_rate': 1.52e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.4553, 'learning_rate': 1.51e-05, 'epoch': 0.24}                      \n",
      "{'loss': 3.4582, 'learning_rate': 1.5e-05, 'epoch': 0.24}                       \n",
      " 70%|█████████████████████████▉           | 3500/5000 [1:07:21<26:37,  1.07s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.19s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.55s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.818758, 'eval_rouge-2': 7.089499999999999, 'eval_rouge-l': 25.199946, 'eval_bleu-4': 0.03540586323248975, 'eval_runtime': 9.9491, 'eval_samples_per_second': 5.026, 'eval_steps_per_second': 0.402, 'epoch': 0.24}\n",
      " 70%|█████████████████████████▉           | 3500/5000 [1:07:31<26:37,  1.07s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.99s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-3500\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-3500/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.6314, 'learning_rate': 1.49e-05, 'epoch': 0.25}                      \n",
      "{'loss': 3.2482, 'learning_rate': 1.48e-05, 'epoch': 0.25}                      \n",
      "{'loss': 3.3971, 'learning_rate': 1.47e-05, 'epoch': 0.25}                      \n",
      "{'loss': 3.4813, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.4297, 'learning_rate': 1.45e-05, 'epoch': 0.25}                      \n",
      "{'loss': 3.4336, 'learning_rate': 1.44e-05, 'epoch': 0.25}                      \n",
      "{'loss': 3.4984, 'learning_rate': 1.43e-05, 'epoch': 0.25}                      \n",
      "{'loss': 3.4207, 'learning_rate': 1.42e-05, 'epoch': 0.25}                      \n",
      "{'loss': 3.4666, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.4387, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.3287, 'learning_rate': 1.3900000000000002e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.3285, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.4955, 'learning_rate': 1.3700000000000001e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.4324, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.4162, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.4766, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.26}        \n",
      "{'loss': 3.4721, 'learning_rate': 1.3300000000000001e-05, 'epoch': 0.26}        \n",
      "{'loss': 3.3525, 'learning_rate': 1.32e-05, 'epoch': 0.26}                      \n",
      "{'loss': 3.3721, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.26}        \n",
      "{'loss': 3.3869, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.26}        \n",
      "{'loss': 3.5006, 'learning_rate': 1.29e-05, 'epoch': 0.26}                      \n",
      "{'loss': 3.4514, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.26}        \n",
      "{'loss': 3.3492, 'learning_rate': 1.27e-05, 'epoch': 0.26}                      \n",
      "{'loss': 3.391, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.26}         \n",
      "{'loss': 3.2633, 'learning_rate': 1.25e-05, 'epoch': 0.26}                      \n",
      "{'loss': 3.4197, 'learning_rate': 1.24e-05, 'epoch': 0.26}                      \n",
      "{'loss': 3.4686, 'learning_rate': 1.23e-05, 'epoch': 0.26}                      \n",
      "{'loss': 3.4971, 'learning_rate': 1.22e-05, 'epoch': 0.26}                      \n",
      "{'loss': 3.4527, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.26}        \n",
      "{'loss': 3.5092, 'learning_rate': 1.2e-05, 'epoch': 0.27}                       \n",
      "{'loss': 3.3857, 'learning_rate': 1.19e-05, 'epoch': 0.27}                      \n",
      "{'loss': 3.3912, 'learning_rate': 1.18e-05, 'epoch': 0.27}                      \n",
      "{'loss': 3.4062, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.27}        \n",
      "{'loss': 3.4686, 'learning_rate': 1.16e-05, 'epoch': 0.27}                      \n",
      "{'loss': 3.4262, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.27}        \n",
      "{'loss': 3.4553, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.27}        \n",
      "{'loss': 3.4422, 'learning_rate': 1.13e-05, 'epoch': 0.27}                      \n",
      "{'loss': 3.5021, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.27}        \n",
      "{'loss': 3.3971, 'learning_rate': 1.11e-05, 'epoch': 0.27}                      \n",
      "{'loss': 3.3492, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.27}        \n",
      "{'loss': 3.4957, 'learning_rate': 1.09e-05, 'epoch': 0.27}                      \n",
      "{'loss': 3.4711, 'learning_rate': 1.08e-05, 'epoch': 0.27}                      \n",
      "{'loss': 3.4295, 'learning_rate': 1.0700000000000001e-05, 'epoch': 0.27}        \n",
      "{'loss': 3.3385, 'learning_rate': 1.06e-05, 'epoch': 0.28}                      \n",
      "{'loss': 3.3668, 'learning_rate': 1.05e-05, 'epoch': 0.28}                      \n",
      "{'loss': 3.5191, 'learning_rate': 1.04e-05, 'epoch': 0.28}                      \n",
      "{'loss': 3.3775, 'learning_rate': 1.03e-05, 'epoch': 0.28}                      \n",
      "{'loss': 3.3967, 'learning_rate': 1.02e-05, 'epoch': 0.28}                      \n",
      "{'loss': 3.4766, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.28}        \n",
      "{'loss': 3.3687, 'learning_rate': 1e-05, 'epoch': 0.28}                         \n",
      " 80%|█████████████████████████████▌       | 4000/5000 [1:16:33<16:37,  1.00it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.45s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.60s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.448698, 'eval_rouge-2': 7.2528179999999995, 'eval_rouge-l': 24.086000000000002, 'eval_bleu-4': 0.03271928188443988, 'eval_runtime': 40.8759, 'eval_samples_per_second': 1.223, 'eval_steps_per_second': 0.098, 'epoch': 0.28}\n",
      " 80%|█████████████████████████████▌       | 4000/5000 [1:17:14<16:37,  1.00it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  7.48s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-4000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-4000/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3846, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.28}         \n",
      "{'loss': 3.3563, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.28}         \n",
      "{'loss': 3.3934, 'learning_rate': 9.7e-06, 'epoch': 0.28}                       \n",
      "{'loss': 3.4221, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.28}         \n",
      "{'loss': 3.3863, 'learning_rate': 9.5e-06, 'epoch': 0.28}                       \n",
      "{'loss': 3.417, 'learning_rate': 9.4e-06, 'epoch': 0.28}                        \n",
      "{'loss': 3.4957, 'learning_rate': 9.3e-06, 'epoch': 0.28}                       \n",
      "{'loss': 3.4016, 'learning_rate': 9.2e-06, 'epoch': 0.28}                       \n",
      "{'loss': 3.4264, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.29}         \n",
      "{'loss': 3.499, 'learning_rate': 9e-06, 'epoch': 0.29}                          \n",
      "{'loss': 3.4197, 'learning_rate': 8.9e-06, 'epoch': 0.29}                       \n",
      "{'loss': 3.4127, 'learning_rate': 8.8e-06, 'epoch': 0.29}                       \n",
      "{'loss': 3.49, 'learning_rate': 8.7e-06, 'epoch': 0.29}                         \n",
      "{'loss': 3.3785, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.29}         \n",
      "{'loss': 3.4383, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.29}         \n",
      "{'loss': 3.4502, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.29}         \n",
      "{'loss': 3.4467, 'learning_rate': 8.3e-06, 'epoch': 0.29}                       \n",
      "{'loss': 3.3758, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.29}         \n",
      "{'loss': 3.4076, 'learning_rate': 8.1e-06, 'epoch': 0.29}                       \n",
      "{'loss': 3.3803, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.29}         \n",
      "{'loss': 3.4201, 'learning_rate': 7.9e-06, 'epoch': 0.29}                       \n",
      "{'loss': 3.4488, 'learning_rate': 7.8e-06, 'epoch': 0.29}                       \n",
      "{'loss': 3.3041, 'learning_rate': 7.7e-06, 'epoch': 0.3}                        \n",
      "{'loss': 3.4121, 'learning_rate': 7.6e-06, 'epoch': 0.3}                        \n",
      "{'loss': 3.4318, 'learning_rate': 7.5e-06, 'epoch': 0.3}                        \n",
      "{'loss': 3.3678, 'learning_rate': 7.4e-06, 'epoch': 0.3}                        \n",
      "{'loss': 3.4629, 'learning_rate': 7.2999999999999996e-06, 'epoch': 0.3}         \n",
      "{'loss': 3.3879, 'learning_rate': 7.2e-06, 'epoch': 0.3}                        \n",
      "{'loss': 3.4801, 'learning_rate': 7.1e-06, 'epoch': 0.3}                        \n",
      "{'loss': 3.4061, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.3}          \n",
      "{'loss': 3.4521, 'learning_rate': 6.900000000000001e-06, 'epoch': 0.3}          \n",
      "{'loss': 3.4656, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.3}          \n",
      "{'loss': 3.4818, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.3}          \n",
      "{'loss': 3.4205, 'learning_rate': 6.6e-06, 'epoch': 0.3}                        \n",
      "{'loss': 3.4133, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.3}         \n",
      "{'loss': 3.4977, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.3}         \n",
      "{'loss': 3.3391, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.31}         \n",
      "{'loss': 3.4154, 'learning_rate': 6.2e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.3797, 'learning_rate': 6.1e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.477, 'learning_rate': 6e-06, 'epoch': 0.31}                          \n",
      "{'loss': 3.4182, 'learning_rate': 5.9e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.4189, 'learning_rate': 5.8e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.4086, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.31}        \n",
      "{'loss': 3.4789, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.31}         \n",
      "{'loss': 3.3951, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.31}         \n",
      "{'loss': 3.3463, 'learning_rate': 5.4e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.5184, 'learning_rate': 5.3e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.5033, 'learning_rate': 5.2e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.4477, 'learning_rate': 5.1e-06, 'epoch': 0.31}                       \n",
      "{'loss': 3.4439, 'learning_rate': 5e-06, 'epoch': 0.31}                         \n",
      " 90%|█████████████████████████████████▎   | 4500/5000 [1:26:17<09:51,  1.18s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.30s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.63s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.478792, 'eval_rouge-2': 6.9715560000000005, 'eval_rouge-l': 25.432361999999998, 'eval_bleu-4': 0.034661887094205895, 'eval_runtime': 9.6772, 'eval_samples_per_second': 5.167, 'eval_steps_per_second': 0.413, 'epoch': 0.31}\n",
      " 90%|█████████████████████████████████▎   | 4500/5000 [1:26:26<09:51,  1.18s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-4500\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-4500/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4375, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.31}        \n",
      "{'loss': 3.3385, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.32}         \n",
      "{'loss': 3.402, 'learning_rate': 4.7e-06, 'epoch': 0.32}                        \n",
      "{'loss': 3.4773, 'learning_rate': 4.6e-06, 'epoch': 0.32}                       \n",
      "{'loss': 3.4176, 'learning_rate': 4.5e-06, 'epoch': 0.32}                       \n",
      "{'loss': 3.3834, 'learning_rate': 4.4e-06, 'epoch': 0.32}                       \n",
      "{'loss': 3.4275, 'learning_rate': 4.2999999999999995e-06, 'epoch': 0.32}        \n",
      "{'loss': 3.2416, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.32}        \n",
      "{'loss': 3.4588, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.32}        \n",
      "{'loss': 3.3893, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.32}         \n",
      "{'loss': 3.4748, 'learning_rate': 3.9e-06, 'epoch': 0.32}                       \n",
      "{'loss': 3.3475, 'learning_rate': 3.8e-06, 'epoch': 0.32}                       \n",
      "{'loss': 3.4004, 'learning_rate': 3.7e-06, 'epoch': 0.32}                       \n",
      "{'loss': 3.4213, 'learning_rate': 3.6e-06, 'epoch': 0.32}                       \n",
      "{'loss': 3.4438, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.32}        \n",
      "{'loss': 3.4572, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.33}        \n",
      "{'loss': 3.409, 'learning_rate': 3.3e-06, 'epoch': 0.33}                        \n",
      "{'loss': 3.3961, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.33}        \n",
      "{'loss': 3.4217, 'learning_rate': 3.1e-06, 'epoch': 0.33}                       \n",
      "{'loss': 3.4174, 'learning_rate': 3e-06, 'epoch': 0.33}                         \n",
      "{'loss': 3.3174, 'learning_rate': 2.9e-06, 'epoch': 0.33}                       \n",
      "{'loss': 3.3576, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.33}        \n",
      "{'loss': 3.4178, 'learning_rate': 2.7e-06, 'epoch': 0.33}                       \n",
      "{'loss': 3.3969, 'learning_rate': 2.6e-06, 'epoch': 0.33}                       \n",
      "{'loss': 3.3076, 'learning_rate': 2.5e-06, 'epoch': 0.33}                       \n",
      "{'loss': 3.4355, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.33}        \n",
      "{'loss': 3.4137, 'learning_rate': 2.3e-06, 'epoch': 0.33}                       \n",
      "{'loss': 3.4117, 'learning_rate': 2.2e-06, 'epoch': 0.33}                       \n",
      "{'loss': 3.3693, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.33}        \n",
      "{'loss': 3.5061, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.34}        \n",
      "{'loss': 3.4254, 'learning_rate': 1.9e-06, 'epoch': 0.34}                       \n",
      "{'loss': 3.4186, 'learning_rate': 1.8e-06, 'epoch': 0.34}                       \n",
      "{'loss': 3.4906, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.34}        \n",
      "{'loss': 3.4197, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.34}        \n",
      "{'loss': 3.4348, 'learning_rate': 1.5e-06, 'epoch': 0.34}                       \n",
      "{'loss': 3.5146, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.34}        \n",
      "{'loss': 3.5295, 'learning_rate': 1.3e-06, 'epoch': 0.34}                       \n",
      "{'loss': 3.4904, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.34}        \n",
      "{'loss': 3.4107, 'learning_rate': 1.1e-06, 'epoch': 0.34}                       \n",
      "{'loss': 3.3713, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.34}        \n",
      "{'loss': 3.368, 'learning_rate': 9e-07, 'epoch': 0.34}                          \n",
      "{'loss': 3.35, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.34}           \n",
      "{'loss': 3.1932, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.34}         \n",
      "{'loss': 3.4865, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.34}         \n",
      "{'loss': 3.4164, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.35}         \n",
      "{'loss': 3.4326, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.35}        \n",
      "{'loss': 3.435, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.35}         \n",
      "{'loss': 3.4609, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.35}        \n",
      "{'loss': 3.4182, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.35}        \n",
      "{'loss': 3.323, 'learning_rate': 0.0, 'epoch': 0.35}                            \n",
      "100%|█████████████████████████████████████| 5000/5000 [1:35:25<00:00,  1.13s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.44s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.53s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.544755999999996, 'eval_rouge-2': 7.205098, 'eval_rouge-l': 24.109968000000002, 'eval_bleu-4': 0.03548614841252268, 'eval_runtime': 30.1257, 'eval_samples_per_second': 1.66, 'eval_steps_per_second': 0.133, 'epoch': 0.35}\n",
      "100%|█████████████████████████████████████| 5000/5000 [1:35:55<00:00,  1.13s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  3.51s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-5000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-5000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5755.438, 'train_samples_per_second': 6.95, 'train_steps_per_second': 0.869, 'train_loss': 3.475465625, 'epoch': 0.35}\n",
      "100%|█████████████████████████████████████| 5000/5000 [1:35:55<00:00,  1.15s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [09:43<00:00,  8.71s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python finetune_hf.py  data/AdvertiseGen_fix  models/ZhipuAI/chatglm3-6b/  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603ed77-d60a-4141-b467-1e2e3870e1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {},
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-2500  checkpoint-4000  checkpoint-5000\n",
      "checkpoint-1500  checkpoint-3000  checkpoint-4500  runs\n",
      "checkpoint-2000  checkpoint-3500  checkpoint-500\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:08<00:00,  1.28s/it]\n",
      "这款连衣裙采用不规则的拼接设计，不规则的剪裁，搭配上不规则的木耳边，让整件连衣裙更加有设计感，同时更加有层次感。再结合百褶裙摆，让整件连衣裙更加灵动，更加有活力。同时采用网纱拼接，让整件连衣裙更加性感，更加有层次感。拉链套头，更加方便穿脱。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0  python inference_hf.py output/checkpoint-5000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {},
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
